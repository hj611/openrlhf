import torch
import json
import os
import requests
from datetime import datetime
from typing import Dict, List, Tuple

# é…ç½®è·¯å¾„
LOG_PATH = os.environ.get("REWARD_LOG_PATH", "/mnt/dolphinfs/ssd_pool/docker/user/hadoop-basecv-hl/hadoop-basecv/huangjing/proj/code/OpenRLHF/reward.log")
URL_MAPPING_PATH = "/mnt/dolphinfs/ssd_pool/docker/user/hadoop-basecv-hl/hadoop-basecv/huangjing/proj/code/OpenRLHF/data/result.json"

# å…¨å±€ç¼“å­˜ URL æ˜ å°„
_url_mapping_cache = None

# ä»£ç†é…ç½®
PROXIES = {
    'http': 'http://10.229.18.23:3128',
    'https': 'http://10.229.18.23:3128'
}

# æ¥å—çš„çŠ¶æ€ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
ACCEPTED_STATUSES = {'accept', 'accepted', 'ac'}


def load_url_mapping(force_reload: bool = False) -> Dict[str, str]:
    """
    åŠ è½½ URL æ˜ å°„é…ç½®
    
    Args:
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
        
    Returns:
        Dict[str, str]: problem_id -> base_url çš„æ˜ å°„
    """
    global _url_mapping_cache
    
    if _url_mapping_cache is None or force_reload:
        try:
            with open(URL_MAPPING_PATH, 'r', encoding='utf-8') as f:
                _url_mapping_cache = json.load(f)
            print(f"âœ“ æˆåŠŸåŠ è½½ URL æ˜ å°„ï¼Œå…± {len(_url_mapping_cache)} ä¸ªé—®é¢˜")
        except FileNotFoundError:
            print(f"âš ï¸  è­¦å‘Š: URL æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ - {URL_MAPPING_PATH}")
            _url_mapping_cache = {}
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯: URL æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯ - {e}")
            _url_mapping_cache = {}
    
    return _url_mapping_cache


def extract_problem_id(answer: str) -> str:
    """
    ä» answer ä¸­æå– problem_id
    
    Args:
        answer: ç­”æ¡ˆå­—ç¬¦ä¸²ï¼ˆå¯èƒ½åŒ…å« problem_idï¼‰
        
    Returns:
        str: æå–çš„ problem_idï¼Œå¦‚æœæå–å¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # æ–¹æ³•1: å¦‚æœ answer æœ¬èº«å°±æ˜¯ problem_id
    if isinstance(answer, str) and '_' in answer:
        return answer.strip()
    
    # æ–¹æ³•2: å¦‚æœ answer æ˜¯ JSON å­—ç¬¦ä¸²
    try:
        answer_data = json.loads(answer)
        if isinstance(answer_data, dict):
            return answer_data.get('problem_id', '')
    except (json.JSONDecodeError, TypeError):
        pass
    
    # æ–¹æ³•3: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
    import re
    match = re.search(r'(\d+_[A-Z])', answer)
    if match:
        return match.group(1)
    
    return ""


def calculate_reward(api_result: Dict) -> Tuple[float, str]:
    """
    æ ¹æ® API è¿”å›ç»“æœè®¡ç®— reward
    
    Args:
        api_result: API è¿”å›çš„ç»“æœå­—å…¸
        
    Returns:
        Tuple[float, str]: (reward å€¼, çŠ¶æ€è¯´æ˜)
    """
    status = api_result.get('status', '').lower().strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¥å—çŠ¶æ€
    if status in ACCEPTED_STATUSES:
        return 1.0, f"âœ… Accepted ({status})"
    else:
        # å…¶ä»–æ‰€æœ‰çŠ¶æ€éƒ½è¿”å› 0
        original_status = api_result.get('status', 'Unknown')
        return 0.0, f"âŒ {original_status}"


def get_reward_from_api(base_url: str, problem_id: str, code: str,  
                        timeout: int = 600) -> Tuple[float, Dict]:
    """
    é€šè¿‡ API è·å– reward
    
    Args:
        base_url: API åŸºç¡€ URL
        problem_id: é—®é¢˜ ID
        code: æäº¤çš„ä»£ç 
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
    Returns:
        Tuple[float, Dict]: (reward å€¼, é¢å¤–ä¿¡æ¯)
    """
    try:
        # æ„å»ºå®Œæ•´ URL
        url = f"{base_url}/api/submit/sync"
        
        # å‡†å¤‡è¯·æ±‚æ•°æ®
        data = {
            "problem_id": problem_id,
            "code": code,
            "language": 'c++17',
        }
        
        print(f"ğŸ”„ å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"   Problem ID: {problem_id}")
        print(f"   Code length: {len(code)} chars")
        
        # å‘é€ POST è¯·æ±‚
        api_response = requests.post(
            url, 
            data=data,
            timeout=timeout,
            proxies=PROXIES,
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        api_response.raise_for_status()
        
        # è§£æå“åº”
        result = api_response.json()
        
        print(f"ğŸ“¥ API å“åº”:")
        print(f"   Status: {result.get('status', 'Unknown')}")
        print(f"   Score: {result.get('score', 0)}")
        print(f"   Time: {result.get('time_used', 0)}ms")
        print(f"   Memory: {result.get('memory_used', 0)}KB")
        
        # è®¡ç®— reward
        reward, status_msg = calculate_reward(result)
        
        print(f"   Reward: {reward} - {status_msg}")
        
        # å¦‚æœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œæ‰“å°å‡ºæ¥
        if result.get('message'):
            print(f"   Message: {result['message'][:200]}...")
        
        # é¢å¤–ä¿¡æ¯
        extra_info = {
            "status": "success",
            "judge_status": result.get('status', 'Unknown'),
            "score": result.get('score', 0),
            "time_used": result.get('time_used', 0),
            "memory_used": result.get('memory_used', 0),
            "message": result.get('message', ''),
            "failed_case": result.get('failed_case', 0),
            "problem_id": problem_id,
            "submission_id": result.get('id', 0),
            "api_response": result
        }
        
        return reward, extra_info
        
    except requests.exceptions.Timeout:
        print(f"âš ï¸  API è¯·æ±‚è¶…æ—¶: {url} - {problem_id}")
        return 0.0, {
            "status": "timeout", 
            "problem_id": problem_id,
            "judge_status": "Timeout"
        }
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ API è¯·æ±‚å¤±è´¥: {e}")
        return 0.0, {
            "status": "error", 
            "error": str(e), 
            "problem_id": problem_id,
            "judge_status": "Request Error"
        }
        
    except (ValueError, KeyError) as e:
        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
        return 0.0, {
            "status": "parse_error", 
            "error": str(e), 
            "problem_id": problem_id,
            "judge_status": "Parse Error"
        }


def reward_func(queries, prompts, labels, **kwargs):
    """
    Reward function for calculating rewards of model outputs.

    Args:
        queries (torch.Tensor or List[str]): Complete text sequences containing prompts and responses (ä»£ç )
        prompts (torch.Tensor or List[str]): Input prompt sequences
        labels (torch.Tensor or List[str]): Ground truth answer sequences (problem_ids)
        **kwargs: Additional optional parameters

    Returns:
        dict: A dictionary containing the following key-value pairs:
            - rewards: Reward values used for calculating advantage function
            - scores: Reward values in range [0,1] used for dynamic filtering
            - extra_logs: Additional information to be logged in wandb
    """
    
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŠ è½½ URL æ˜ å°„
    url_mapping = load_url_mapping()
    
    # å­˜å‚¨æ‰€æœ‰ reward
    rewards_list = []
    extra_logs_list = []
    
    # ç»Ÿè®¡ä¿¡æ¯
    status_counter = {}
    
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    print(f"ğŸ“ æ—¥å¿—è·¯å¾„: {LOG_PATH}")
    with open(LOG_PATH, "a", encoding='utf-8') as f:
        f.write(f"\n{'='*80}\n")
        f.write(f"Batch Evaluation - {current_time}\n")
        f.write(f"{'='*80}\n\n")
        
        # éå†æ¯ä¸ªæ ·æœ¬
        for idx, (query, prompt, answer) in enumerate(zip(queries, prompts, labels)):
            f.write(f"\n{'â”€'*80}\n")
            f.write(f"Sample {idx + 1}/{len(queries)}\n")
            f.write(f"{'â”€'*80}\n")
            
            # è§£æ promptï¼ˆå¦‚æœæ˜¯ JSONï¼‰
            prompt_data = None
            try:
                if isinstance(prompt, str):
                    prompt_data = json.loads(prompt)
                    f.write(f"âœ“ Prompt è§£ææˆåŠŸ\n")
                    f.write(f"Prompt Data: {json.dumps(prompt_data, indent=2, ensure_ascii=False)}\n\n")
            except json.JSONDecodeError as e:
                f.write(f"âš ï¸  Prompt JSON è§£ç é”™è¯¯: {e}\n")
                f.write(f"Raw Prompt: {prompt[:200]}...\n\n")
            
            # æå– problem_id
            problem_id = extract_problem_id(answer)
            
            if not problem_id:
                f.write(f"âŒ æ— æ³•æå– problem_id from answer: {answer}\n")
                rewards_list.append(0.0)
                extra_logs_list.append({
                    "status": "no_problem_id", 
                    "answer": str(answer),
                    "judge_status": "Invalid Problem ID"
                })
                status_counter["Invalid Problem ID"] = status_counter.get("Invalid Problem ID", 0) + 1
                continue
            
            f.write(f"Problem ID: {problem_id}\n")
            
            # æŸ¥æ‰¾å¯¹åº”çš„ base_url
            base_url = url_mapping.get(problem_id)
            
            if not base_url:
                f.write(f"âš ï¸  æœªæ‰¾åˆ° problem_id å¯¹åº”çš„ URL: {problem_id}\n")
                f.write(f"å¯ç”¨çš„ problem_ids: {list(url_mapping.keys())[:10]}...\n")
                rewards_list.append(0.0)
                extra_logs_list.append({
                    "status": "url_not_found", 
                    "problem_id": problem_id,
                    "judge_status": "URL Not Found"
                })
                status_counter["URL Not Found"] = status_counter.get("URL Not Found", 0) + 1
                continue
            
            f.write(f"Base URL: {base_url}\n")
            
            # query å°±æ˜¯ä»£ç 
            code = str(query).strip()
            
            f.write(f"\n===the gen Code: {code}\n")
            # f.write(f"Code length: {len(code)} chars\n")
            f.write(f"Answer (Problem ID): {answer}\n\n")
            
            # è°ƒç”¨ API è·å– reward
            f.write(f"ğŸ”„ æ­£åœ¨è¯·æ±‚ API...\n")
            reward, extra_info = get_reward_from_api(
                base_url=base_url,
                problem_id=problem_id,
                code=code
            )
            
            f.write(f"âœ“ Reward: {reward}\n")
            f.write(f"Judge Status: {extra_info.get('judge_status', 'Unknown')}\n")
            f.write(f"Score: {extra_info.get('score', 0)}\n")
            f.write(f"Time Used: {extra_info.get('time_used', 0)}ms\n")
            f.write(f"Memory Used: {extra_info.get('memory_used', 0)}KB\n")
            
            if extra_info.get('message'):
                f.write(f"Message: {extra_info['message'][:500]}...\n")
            
            f.write(f"\nExtra Info: {json.dumps(extra_info, indent=2, ensure_ascii=False)}\n")
            
            rewards_list.append(reward)
            extra_logs_list.append(extra_info)
            
            # ç»Ÿè®¡çŠ¶æ€
            judge_status = extra_info.get('judge_status', 'Unknown')
            status_counter[judge_status] = status_counter.get(judge_status, 0) + 1
    
    # è½¬æ¢ä¸º tensor
    rewards_tensor = torch.tensor(rewards_list, dtype=torch.float32)
    
    # scores å°±æ˜¯ rewardsï¼ˆå› ä¸ºå·²ç»æ˜¯ 0 æˆ– 1ï¼‰
    scores_tensor = rewards_tensor.clone()
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(rewards_list)
    accepted_count = sum(1 for r in rewards_list if r > 0)
    avg_reward = sum(rewards_list) / total_samples if total_samples > 0 else 0
    success_rate = (accepted_count / total_samples * 100) if total_samples > 0 else 0
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Batch Evaluation Summary - {current_time}")
    print(f"{'='*80}")
    print(f"Total samples: {total_samples}")
    print(f"Accepted: {accepted_count} ({success_rate:.2f}%)")
    print(f"Failed: {total_samples - accepted_count} ({100 - success_rate:.2f}%)")
    print(f"Average reward: {avg_reward:.4f}")
    print(f"\nçŠ¶æ€åˆ†å¸ƒ:")
    for status, count in sorted(status_counter.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_samples * 100) if total_samples > 0 else 0
        print(f"  {status}: {count} ({percentage:.2f}%)")
    print(f"{'='*80}\n")
    
    return {
        "rewards": rewards_tensor,  # Rewards for advantage calculation (0 or 1)
        "scores": scores_tensor,    # Scores for dynamic filtering (0 or 1)
        "extra_logs": {
            "reward_details": extra_logs_list,
            "avg_reward": avg_reward,
            "max_reward": 1.0,
            "min_reward": 0.0,
            "accepted_count": accepted_count,
            "total_count": total_samples,
            "success_rate": success_rate,
            "status_distribution": status_counter
        }
    }


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹æµ‹è¯• reward_func\n")
    
    # æµ‹è¯•æ•°æ®
    test_queries = [
        # æµ‹è¯•1: æœ‰ç¼–è¯‘é”™è¯¯çš„ä»£ç ï¼ˆåº”è¯¥è¿”å› 0ï¼‰
        "#include <bits/stdc++.h>\n#pragma comment(linker, \"/STACK:2000000\")\n#pragma comment(linker, \"/HEAP:2000000\")\nusing namespace std;\nint32_t main() {\n  ios_base::sync_with_stdio(false);\n  cin.tie(NULL);\n  cout.tie(NULL);\n  int n;\n  cin >> n;\n  long long m[n][n], a[n];\n  for (int i = 0; i < n; i++) {\n    for (int j = 0; j < n; j++) {\n      cin >> m[i][j];\n    }\n  }\n  long long x = sqrt((m[0][1] * m[0][2]) \\/ m[1][2]);\n  cout << x << \" \";\n  for (int i = 1; i < n; i++) {\n    cout << m[0][i] \\/ x << \" \";\n  }\n  return 0;\n}\n",
        
        # æµ‹è¯•2: æ­£ç¡®çš„ä»£ç ï¼ˆåº”è¯¥è¿”å› 1ï¼Œå¦‚æœé€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼‰
        "#include <bits/stdc++.h>\nusing namespace std;\nint main() {\n  int n;\n  cin >> n;\n  long long m[n][n];\n  for (int i = 0; i < n; i++) {\n    for (int j = 0; j < n; j++) {\n      cin >> m[i][j];\n    }\n  }\n  long long x = sqrt((m[0][1] * m[0][2]) / m[1][2]);\n  cout << x << \" \";\n  for (int i = 1; i < n; i++) {\n    cout << m[0][i] / x << \" \";\n  }\n  return 0;\n}\n",
    ]
    
    test_prompts = [
        '{"problem_id": "1220_B", "description": "..."}',
        '{"problem_id": "1220_B", "description": "..."}',
    ]
    
    test_labels = [
        "1220_B",
        "1220_B",
    ]
    
    # è°ƒç”¨ reward å‡½æ•°
    result = reward_func(test_queries, test_prompts, test_labels)
    
    print("\n" + "="*80)
    print("ğŸ¯ æµ‹è¯•ç»“æœ:")
    print("="*80)
    print(f"Rewards: {result['rewards']}")
    print(f"Scores: {result['scores']}")
    print(f"\nExtra logs:")
    print(f"  Average reward: {result['extra_logs']['avg_reward']:.4f}")
    print(f"  Accepted: {result['extra_logs']['accepted_count']}/{result['extra_logs']['total_count']}")
    print(f"  Success rate: {result['extra_logs']['success_rate']:.2f}%")
    print(f"\n  Status distribution:")
    for status, count in result['extra_logs']['status_distribution'].items():
        print(f"    {status}: {count}")
    print("="*80)