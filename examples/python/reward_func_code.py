import torch
import json
import os
import requests
import re
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional

# ============================================================================
# é…ç½®
# ============================================================================
LOG_PATH = os.environ.get(
    "REWARD_LOG_PATH", 
    "/mnt/dolphinfs/ssd_pool/docker/user/hadoop-basecv-hl/hadoop-basecv/huangjing/proj/code/OpenRLHF/reward.log"
)
URL_MAPPING_PATH = "/mnt/dolphinfs/ssd_pool/docker/user/hadoop-basecv-hl/hadoop-basecv/huangjing/proj/code/OpenRLHF/data/result.json"

# å…¨å±€ç¼“å­˜ URL æ˜ å°„
_url_mapping_cache = None

# ä»£ç†é…ç½®
PROXIES = {
    'http': 'http://10.229.18.23:3128',
    'https': 'http://10.229.18.23:3128'
}

# æ¥å—çš„çŠ¶æ€ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
ACCEPTED_STATUSES = {'accept', 'accepted', 'ac'}

problem_pattern = r"<\|im_start\|>user\n(.*?)<\|im_end\|>"
response_prefix = r"<\|im_start\|>assistant\n"

# ğŸ”§ æ·»åŠ ï¼šæ¯ä¸ªæœåŠ¡å™¨çš„è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
REQUEST_INTERVAL_PER_SERVER = 1.0  # åŒä¸€æœåŠ¡å™¨çš„è¯·æ±‚é—´éš” 1 ç§’
_last_request_time_per_server = {}  # è®°å½•æ¯ä¸ªæœåŠ¡å™¨çš„æœ€åè¯·æ±‚æ—¶é—´


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def load_url_mapping(force_reload: bool = False) -> Dict[str, str]:
    """åŠ è½½ URL æ˜ å°„é…ç½®"""
    global _url_mapping_cache
    
    if _url_mapping_cache is None or force_reload:
        try:
            with open(URL_MAPPING_PATH, 'r', encoding='utf-8') as f:
                _url_mapping_cache = json.load(f)
            print(f"âœ“ æˆåŠŸåŠ è½½ URL æ˜ å°„ï¼Œå…± {len(_url_mapping_cache)} ä¸ªé—®é¢˜")
        except FileNotFoundError:
            print(f"âš ï¸  è­¦å‘Š: URL æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ - {URL_MAPPING_PATH}")
            _url_mapping_cache = {}
        except json.JSONDecodeError as e:
            print(f"âŒ é”™è¯¯: URL æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯ - {e}")
            _url_mapping_cache = {}
    
    return _url_mapping_cache


def get_response_from_query(q: str):
    ends_of_sentence = ["<|im_end|>", "<ï½œendâ–ofâ–sentenceï½œ>", "<|endoftext|>"]
    pos = re.search(response_prefix, q)
    if pos is None:
        return ""
    response = q[pos.end():]
    for e in ends_of_sentence:
        response = response.replace(e, "")
    return response.strip()


def extract_code_from_output(text: str) -> Optional[str]:
    """ä»æ¨¡å‹è¾“å‡ºä¸­æå–ä»£ç å—"""
    
    # æ–¹æ³•1: æå– Markdown ä»£ç å—ï¼ˆå¸¦è¯­è¨€æ ‡è®°ï¼‰
    pattern1 = r'```(?:cpp|c\+\+|c|python|java|javascript|go|rust)\s*\n(.*?)```'
    matches = re.findall(pattern1, text, re.DOTALL | re.IGNORECASE)
    
    if matches:
        code = matches[-1].strip()
        print(f"âœ“ æå–åˆ°ä»£ç å—ï¼ˆæ–¹æ³•1ï¼‰: {len(code)} å­—ç¬¦")
        return code
    
    # æ–¹æ³•2: æå–æ— è¯­è¨€æ ‡è®°çš„ä»£ç å—
    pattern2 = r'```\s*\n(.*?)```'
    matches = re.findall(pattern2, text, re.DOTALL)
    
    if matches:
        code = matches[-1].strip()
        print(f"âœ“ æå–åˆ°ä»£ç å—ï¼ˆæ–¹æ³•2ï¼‰: {len(code)} å­—ç¬¦")
        return code
    
    # æ–¹æ³•3: æŸ¥æ‰¾ #include å¼€å¤´çš„ C++ ä»£ç 
    pattern3 = r'(#include\s+<[^>]+>.*?)(?:\n\n[A-Z]|\Z)'
    matches = re.findall(pattern3, text, re.DOTALL)
    
    if matches:
        code = matches[-1].strip()
        print(f"âœ“ æå–åˆ°ä»£ç å—ï¼ˆæ–¹æ³•3ï¼‰: {len(code)} å­—ç¬¦")
        return code
    
    print("âš ï¸  æœªèƒ½æå–åˆ°ä»£ç å—")
    return None


def extract_problem_id(answer: str) -> str:
    """ä» answer ä¸­æå– problem_id"""
    if isinstance(answer, str) and '_' in answer:
        return answer.strip()
    
    try:
        answer_data = json.loads(answer)
        if isinstance(answer_data, dict):
            return answer_data.get('problem_id', '')
    except (json.JSONDecodeError, TypeError):
        pass
    
    match = re.search(r'(\d+_[A-Z])', answer)
    if match:
        return match.group(1)
    
    return ""


def calculate_reward(api_result: Dict) -> Tuple[float, str]:
    """æ ¹æ® API è¿”å›ç»“æœè®¡ç®— reward"""
    status = api_result.get('status', '').lower().strip()
    
    if status in ACCEPTED_STATUSES:
        return 1.0, f"âœ… Accepted ({status})"
    else:
        original_status = api_result.get('status', 'Unknown')
        return 0.0, f"âŒ {original_status}"

import random  # å¯¼å…¥ random æ¨¡å—

RANDOM_WAIT_MIN = 0.0  # æœ€å°ç­‰å¾…æ—¶é—´
RANDOM_WAIT_MAX = 1.0  # æœ€å¤§ç­‰å¾…æ—¶é—´

def random_wait():
    """éšæœºç­‰å¾… 0-1 ç§’"""
    wait_time = random.uniform(RANDOM_WAIT_MIN, RANDOM_WAIT_MAX)
    print(f"ğŸ² éšæœºç­‰å¾… {wait_time:.3f} ç§’...")
    time.sleep(wait_time)


def get_reward_from_api(
    base_url: str, 
    problem_id: str, 
    code: str,  
    timeout: int = 600
) -> Tuple[float, Dict]:
    """
    é€šè¿‡ API è·å– rewardï¼ˆä¸²è¡Œï¼Œå¸¦é—´éš”æ§åˆ¶ï¼‰
    """
    try:
        
        url = f"{base_url}/api/submit/sync"
        
        data = {
            "problem_id": problem_id,
            "code": code,
            "language": 'c++17',
        }
        
        print(f"ğŸ”„ å‘é€è¯·æ±‚åˆ°: {url}")
        print(f"   Problem ID: {problem_id}")
        print(f"   Code length: {len(code)} chars")
        print(f"   æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        
        # ğŸ”§ å‘é€è¯·æ±‚å¹¶ç­‰å¾…å“åº”
        request_start_time = time.time()
        
        api_response = requests.post(
            url, 
            data=data,
            timeout=timeout,
            proxies=PROXIES,
        )
        
        request_duration = time.time() - request_start_time
        
        api_response.raise_for_status()
        result = api_response.json()
        
        print(f"ğŸ“¥ API å“åº” (è€—æ—¶ {request_duration:.2f}s):")
        print(f"   Status: {result.get('status', 'Unknown')}")
        print(f"   Score: {result.get('score', 0)}")
        
        reward, status_msg = calculate_reward(result)
        print(f"   Reward: {reward} - {status_msg}")
        
        # è¿”å›æ ‡é‡å€¼
        extra_info = {
            "status": "success",
            "judge_status": result.get('status', 'Unknown'),
            "score": float(result.get('score', 0)),
            "time_used": float(result.get('time_used', 0)),
            "memory_used": float(result.get('memory_used', 0)),
            "failed_case": float(result.get('failed_case', 0)),
            "problem_id": problem_id,
            "submission_id": float(result.get('id', 0)),
            "request_duration": request_duration,  # ğŸ”§ è®°å½•è¯·æ±‚è€—æ—¶
        }
        
        return reward, extra_info
        
    except requests.exceptions.Timeout:
        print(f"âš ï¸  API è¯·æ±‚è¶…æ—¶ (>{timeout}s)")
        return 0.0, {
            "status": "timeout", 
            "judge_status": "Timeout",
            "score": 0.0,
            "time_used": 0.0,
            "memory_used": 0.0,
            "failed_case": 0.0,
            "problem_id": problem_id,
            "submission_id": 0.0,
            "request_duration": timeout,
        }
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ API è¯·æ±‚å¤±è´¥: {e}")
        return 0.0, {
            "status": "error", 
            "judge_status": "Request Error",
            "score": 0.0,
            "time_used": 0.0,
            "memory_used": 0.0,
            "failed_case": 0.0,
            "problem_id": problem_id,
            "submission_id": 0.0,
            "request_duration": 0.0,
        }
        
    except (ValueError, KeyError) as e:
        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
        return 0.0, {
            "status": "parse_error", 
            "judge_status": "Parse Error",
            "score": 0.0,
            "time_used": 0.0,
            "memory_used": 0.0,
            "failed_case": 0.0,
            "problem_id": problem_id,
            "submission_id": 0.0,
            "request_duration": 0.0,
        }


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def reward_func(queries, prompts, labels, **kwargs):
    """
    Reward function for calculating rewards of model outputs.

    çœŸæ­£å¹¶è¡Œå¤„ç†ï¼šå°†æ ·æœ¬æŒ‰æœåŠ¡å™¨åˆ†ç»„ï¼Œæ¯å°æœåŠ¡å™¨ç‹¬ç«‹ä¸²è¡Œå¤„ç†è‡ªå·±çš„é˜Ÿåˆ—ã€‚

    Args:
        queries: æ¨¡å‹çš„å®Œæ•´è¾“å‡ºï¼ˆåŒ…å«ä»£ç ï¼‰
        prompts: è¾“å…¥æç¤º
        labels: æ ‡å‡†ç­”æ¡ˆï¼ˆproblem_idï¼‰

    Returns:
        dict: {
            "rewards": Tensor[batch_size],
            "scores": Tensor[batch_size],
            "extra_logs": Dict[str, Tensor[batch_size]]
        }
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import threading

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    url_mapping = load_url_mapping()

    batch_size = len(queries)

    # æ”¶é›†æŒ‡æ ‡ - ä½¿ç”¨å­—å…¸æŒ‰ç´¢å¼•å­˜å‚¨ï¼Œç¡®ä¿é¡ºåº
    results = {}
    results_lock = threading.Lock()

    # ç»Ÿè®¡ä¿¡æ¯
    status_counter = {}
    code_extraction_stats = {"success": 0, "failed": 0}
    stats_lock = threading.Lock()

    # ä»url_mappingä¸­æå–æ‰€æœ‰å”¯ä¸€çš„base_url
    unique_base_urls = list(set(url_mapping.values()))

    # æŒ‰æœåŠ¡å™¨åˆ†ç»„æ ·æœ¬
    server_samples = {url: [] for url in unique_base_urls}

    for idx, (query, prompt, answer) in enumerate(zip(queries, prompts, labels)):
        problem_id = extract_problem_id(answer)
        if problem_id:
            base_url = url_mapping.get(problem_id)
            if base_url and base_url in server_samples:
                server_samples[base_url].append((idx, query, prompt, answer))

    print(f"ğŸ“ å¤„ç† {batch_size} ä¸ªæ ·æœ¬ï¼ˆçœŸå¹¶è¡Œæ¨¡å¼ï¼Œ{len(unique_base_urls)} å°æœåŠ¡å™¨ï¼‰")
    print(f"ğŸ“ æœåŠ¡å™¨åˆ—è¡¨: {unique_base_urls}")
    for url, samples in server_samples.items():
        print(f"   {url}: {len(samples)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ æ—¥å¿—è·¯å¾„: {LOG_PATH}")
    print(f"ğŸ“ æ¯å°æœåŠ¡å™¨è¯·æ±‚é—´éš”: {REQUEST_INTERVAL_PER_SERVER} ç§’")

    batch_start_time = time.time()

    def process_server_queue(base_url, samples):
        """å¤„ç†å•å°æœåŠ¡å™¨çš„æ‰€æœ‰æ ·æœ¬ï¼ˆä¸²è¡Œï¼‰"""
        log_entries = []

        log_entries.append(f"\n{'='*80}\n")
        log_entries.append(f"Server: {base_url}\n")
        log_entries.append(f"Samples: {len(samples)}\n")
        log_entries.append(f"{'='*80}\n")

        for idx, query, prompt, answer in samples:
            sample_start_time = time.time()

            log_entries.append(f"\n{'â”€'*80}\n")
            log_entries.append(f"Sample {idx + 1}/{batch_size} - {datetime.now().strftime('%H:%M:%S')}\n")
            log_entries.append(f"Server: {base_url}\n")
            log_entries.append(f"{'â”€'*80}\n")

            response = get_response_from_query(query)

            if not response:
                print(f"âš ï¸  [{base_url}] æ ·æœ¬ {idx}: æœªèƒ½æå–åˆ° assistant å›å¤")
                result = {
                    "reward": 0.0,
                    "score": 0.0,
                    "time_used": 0.0,
                    "memory_used": 0.0,
                    "failed_case": 0.0,
                    "submission_id": 0.0,
                    "request_duration": 0.0,
                    "judge_status": "No Response"
                }
                with results_lock:
                    results[idx] = result
                with stats_lock:
                    status_counter["No Response"] = status_counter.get("No Response", 0) + 1
                continue

            # æå– problem_id
            problem_id = extract_problem_id(answer)

            if not problem_id:
                log_entries.append(f"âŒ æ— æ³•æå– problem_id from answer: {answer}\n")
                result = {
                    "reward": 0.0,
                    "score": 0.0,
                    "time_used": 0.0,
                    "memory_used": 0.0,
                    "failed_case": 0.0,
                    "submission_id": 0.0,
                    "request_duration": 0.0,
                    "judge_status": "Invalid Problem ID"
                }
                with results_lock:
                    results[idx] = result
                with stats_lock:
                    status_counter["Invalid Problem ID"] = status_counter.get("Invalid Problem ID", 0) + 1
                continue

            log_entries.append(f"Problem ID: {problem_id}\n")

            # ä»è¾“å‡ºä¸­æå–ä»£ç 
            full_output = str(response).strip()
            code = extract_code_from_output(full_output)

            if code is None:
                log_entries.append(f"âŒ æœªèƒ½ä»è¾“å‡ºä¸­æå–ä»£ç å—\n")
                code = full_output
                with stats_lock:
                    code_extraction_stats["failed"] += 1
            else:
                log_entries.append(f"âœ“ æˆåŠŸæå–ä»£ç å—\n")
                with stats_lock:
                    code_extraction_stats["success"] += 1

            log_entries.append(f"\n===full_output: \n{full_output[:500]}...\n\n")
            log_entries.append(f"Code length: {len(code)} chars\n")
            log_entries.append(f"\n===Extracted Code:\n{code[:500]}...\n\n")

            # é™æµï¼šç­‰å¾…é—´éš”
            time.sleep(REQUEST_INTERVAL_PER_SERVER)

            log_entries.append(f"ğŸ”„ æ­£åœ¨è¯·æ±‚ API...\n")

            reward, extra_info = get_reward_from_api(
                base_url=base_url,
                problem_id=problem_id,
                code=code
            )

            sample_duration = time.time() - sample_start_time

            log_entries.append(f"âœ“ Reward: {reward}\n")
            log_entries.append(f"Judge Status: {extra_info.get('judge_status', 'Unknown')}\n")
            log_entries.append(f"Score: {extra_info.get('score', 0)}\n")
            log_entries.append(f"Time Used: {extra_info.get('time_used', 0)} ms\n")
            log_entries.append(f"Memory Used: {extra_info.get('memory_used', 0)} KB\n")
            log_entries.append(f"Request Duration: {extra_info.get('request_duration', 0):.2f}s\n")
            log_entries.append(f"Sample Total Duration: {sample_duration:.2f}s\n")

            # ä¿å­˜ç»“æœ
            result = {
                "reward": reward,
                "score": extra_info['score'],
                "time_used": extra_info['time_used'],
                "memory_used": extra_info['memory_used'],
                "failed_case": extra_info['failed_case'],
                "submission_id": extra_info['submission_id'],
                "request_duration": extra_info.get('request_duration', 0),
                "judge_status": extra_info.get('judge_status', 'Unknown')
            }

            with results_lock:
                results[idx] = result

            with stats_lock:
                judge_status = extra_info.get('judge_status', 'Unknown')
                status_counter[judge_status] = status_counter.get(judge_status, 0) + 1

            print(f"âœ“ [{base_url}] å®Œæˆ {idx + 1}/{batch_size} (è€—æ—¶ {sample_duration:.2f}s)")

        return log_entries

    # å¹¶è¡Œå¤„ç†ï¼šæ¯å°æœåŠ¡å™¨ä¸€ä¸ªçº¿ç¨‹
    with open(LOG_PATH, "a", encoding='utf-8') as f:
        f.write(f"\n{'='*80}\n")
        f.write(f"Batch Evaluation (True Parallel) - {current_time}\n")
        f.write(f"Batch Size: {batch_size}\n")
        f.write(f"Servers: {len(unique_base_urls)}\n")
        f.write(f"Request Interval per Server: {REQUEST_INTERVAL_PER_SERVER}s\n")
        f.write(f"{'='*80}\n\n")

        # ä¸ºæ¯å°æœåŠ¡å™¨å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹
        with ThreadPoolExecutor(max_workers=len(unique_base_urls)) as executor:
            futures = {
                executor.submit(process_server_queue, base_url, samples): base_url
                for base_url, samples in server_samples.items()
                if samples  # åªå¤„ç†æœ‰æ ·æœ¬çš„æœåŠ¡å™¨
            }

            # æ”¶é›†æ—¥å¿—
            for future in as_completed(futures):
                base_url = futures[future]
                try:
                    log_entries = future.result()
                    f.writelines(log_entries)
                    f.flush()
                    print(f"âœ“ æœåŠ¡å™¨ {base_url} å®Œæˆæ‰€æœ‰æ ·æœ¬")
                except Exception as e:
                    print(f"âŒ æœåŠ¡å™¨ {base_url} å¤„ç†å¤±è´¥: {e}")
                    f.write(f"âŒ Server {base_url} failed: {e}\n")

    batch_duration = time.time() - batch_start_time

    # æŒ‰ç´¢å¼•é¡ºåºæå–ç»“æœ
    rewards_list = [results.get(i, {"reward": 0.0})["reward"] for i in range(batch_size)]
    scores_list = [results.get(i, {"score": 0.0})["score"] for i in range(batch_size)]
    time_used_list = [results.get(i, {"time_used": 0.0})["time_used"] for i in range(batch_size)]
    memory_used_list = [results.get(i, {"memory_used": 0.0})["memory_used"] for i in range(batch_size)]
    failed_case_list = [results.get(i, {"failed_case": 0.0})["failed_case"] for i in range(batch_size)]
    submission_id_list = [results.get(i, {"submission_id": 0.0})["submission_id"] for i in range(batch_size)]
    request_duration_list = [results.get(i, {"request_duration": 0.0})["request_duration"] for i in range(batch_size)]

    # è½¬æ¢ä¸º tensor
    rewards_tensor = torch.tensor(rewards_list, dtype=torch.float32)
    scores_tensor = torch.tensor(scores_list, dtype=torch.float32)

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    accepted_count = sum(1 for r in rewards_list if r > 0)
    avg_reward = sum(rewards_list) / batch_size if batch_size > 0 else 0
    success_rate = (accepted_count / batch_size * 100) if batch_size > 0 else 0
    avg_request_duration = sum(request_duration_list) / len(request_duration_list) if request_duration_list else 0

    # è®¡ç®—ç†è®ºä¸²è¡Œæ—¶é—´å’Œå®é™…åŠ é€Ÿæ¯”
    total_request_time = sum(request_duration_list)
    theoretical_speedup = total_request_time / batch_duration if batch_duration > 0 else 1

    # ç»Ÿè®¡æ¯å°æœåŠ¡å™¨çš„ä½¿ç”¨æƒ…å†µ
    server_usage = {}
    for base_url, samples in server_samples.items():
        server_usage[base_url] = len(samples)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Batch Evaluation Summary - {current_time}")
    print(f"{'='*80}")
    print(f"Total samples: {batch_size}")
    print(f"Batch duration: {batch_duration:.2f}s")
    print(f"Total request time: {total_request_time:.2f}s")
    print(f"Average request duration: {avg_request_duration:.2f}s")
    print(f"Actual speedup: {theoretical_speedup:.2f}x")
    print(f"Accepted: {accepted_count} ({success_rate:.2f}%)")
    print(f"Average reward: {avg_reward:.4f}")
    print(f"\nä»£ç æå–ç»Ÿè®¡:")
    print(f"  æˆåŠŸ: {code_extraction_stats['success']}")
    print(f"  å¤±è´¥: {code_extraction_stats['failed']}")
    print(f"\næœåŠ¡å™¨ä½¿ç”¨åˆ†å¸ƒ:")
    for server, count in sorted(server_usage.items()):
        percentage = (count / batch_size * 100) if batch_size > 0 else 0
        print(f"  {server}: {count} ({percentage:.2f}%)")
    print(f"\nçŠ¶æ€åˆ†å¸ƒ:")
    for status, count in sorted(status_counter.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / batch_size * 100) if batch_size > 0 else 0
        print(f"  {status}: {count} ({percentage:.2f}%)")
    print(f"{'='*80}\n")

    return {
        "rewards": rewards_tensor,
        "scores": scores_tensor,
        "extra_logs": {
            "time_used": torch.tensor(time_used_list, dtype=torch.float32),
            "memory_used": torch.tensor(memory_used_list, dtype=torch.float32),
            "failed_case": torch.tensor(failed_case_list, dtype=torch.float32),
            "submission_id": torch.tensor(submission_id_list, dtype=torch.float32),
            "request_duration": torch.tensor(request_duration_list, dtype=torch.float32),
        }
    }


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    print("ğŸ§ª å¼€å§‹æµ‹è¯• reward_func (ä¸²è¡Œæ¨¡å¼)\n")
    
    test_query1 = """<|im_start|>assistant
```cpp
#include <bits/stdc++.h>
using namespace std;

int main() {
    int n;
    cin >> n;
    cout << n << endl;
    return 0;
}
```<|im_end|>"""
    
    test_queries = [test_query1, test_query1]
    test_prompts = ['{"problem_id": "1220_B"}', '{"problem_id": "1220_B"}']
    test_labels = ["1220_B", "1220_B"]
    
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_queries)}\n")
    
    result = reward_func(test_queries, test_prompts, test_labels)
    
    print("\nğŸ¯ æµ‹è¯•ç»“æœ:")
    print(f"Rewards: {result['rewards']}")
    print(f"Request durations: {result['extra_logs']['request_duration']}")